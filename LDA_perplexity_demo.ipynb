{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is mostly a \"fun\" excercise using topic modelling (more precisely Latent Dirichlet Allocation, LDA)  and Self Organizing Maps (SOMs).  \n",
    "\n",
    "It is not my intention to explain LDA here. There are a wealth of resources out there for that. One that I particularly like is [this tutorial](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf) of the R package `topicmodels`. Even if you don't code in R, the writting is very nice and clear. For example, taken directly from that document: the generative model consists of the following three steps. \n",
    "\n",
    "1. The term distribution $\\beta$ is determined for each topic by: \n",
    "\n",
    "    $\\beta \\sim$ Dirichlet($\\delta$)\n",
    "\n",
    "    $\\beta$ is the term distribution of topics and contains the probability of a word occurring\n",
    "    in a given topic\n",
    "\n",
    "2. The proportions $\\theta$ of the topic distribution for the document $w$ are determined by\n",
    "\n",
    "    $\\theta \\sim$  Dirichlet($\\alpha$).\n",
    "\n",
    "3. For each of the N words $w_i$\n",
    "\n",
    "    (a) Choose a topic $z_i \\sim$ Multinomial($\\theta$).\n",
    "    \n",
    "    (b) Choose a word $w_i$ from a multinomial probability distribution conditioned on the topic $z_i$: \n",
    "    \n",
    "    $p(w_i|z_i, \\beta)$.\n",
    "\n",
    "\n",
    "So...in (perhaps too) \"simpler\" words. We are trying to \"reconstruct\" a document based on the most probable words associated to the topics present in that document. All this thorugh an EM algorithm.\n",
    "\n",
    "For this excercise I will use the well known and easily tractable [20newsgroup](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html) dataset. In the \"near\" (we'll see how near...) future we intend to do something more sophisticated with a [larger dataset](https://github.com/fabianmurariu/website-categories-nn) with content for nearly a million urls. \n",
    "\n",
    "Anyway, let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/ubuntu/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import os,sys,re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# wherever you place your dataset\n",
    "TEXT_DATA_DIR = '/home/ubuntu/working/text_classification/20_newsgroup/'\n",
    "\n",
    "docs = []\n",
    "doc_classes = []\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath)\n",
    "                t = f.read()\n",
    "                # skip header\n",
    "                i = t.find('\\n\\n')\n",
    "                if 0 < i:\n",
    "                    t = t[i:]\n",
    "                t = BeautifulSoup(t).get_text()\n",
    "                t = re.sub(\"[^a-zA-Z]\",\" \", t)\n",
    "                docs.append(t)\n",
    "                doc_classes.append(name)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive name  atheism resources Alt atheism archive name  resources Last modified     December      Version                                     Atheist Resources                        Addresses of Atheist Organizations                                       USA  FREEDOM FROM RELIGION FOUNDATION  Darwin fish bumper stickers and assorted other atheist paraphernalia are available from the Freedom From Religion Foundation in the US   Write to   FFRF  P O  Box      Madison  WI        Telephone                  EVOLUTION DESIGNS  Evolution Designs sell the  Darwin fish    It s a fish symbol  like the ones Christians stick on their cars  but with feet and the word  Darwin  written inside   The deluxe moulded  D plastic fish is       postpaid in the US   Write to   Evolution Designs       Laurel Canyon     North Hollywood             CA         People in the San Francisco Bay area can get Darwin Fish from Lynn Gold    try mailing    For net people who go to Lynn directly  the price is       per\n"
     ]
    }
   ],
   "source": [
    "# let's print the first ~1000 characters of the 1st document\n",
    "print(docs[0][:1001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS\n",
    "\n",
    "We are going to run 3 experiments applying different preprocessing routines to the documents before passing them to the LDA model. We will evaluate each set up using the probability-based metric [perplexity](http://qpleple.com/perplexity-to-evaluate-topic-models/) (I will come back to this metric later). In the `lda_perplexity.py` script the experiments are wrapped up in functions like this\n",
    "    \n",
    "```\n",
    "def experiment_1(docs, tokenizer_):\n",
    "    train_docs, test_docs = train_test_split(docs, test_size=0.25, random_state=0)\n",
    "    vectorizer = CountVectorizer(min_df=10, max_df=0.5,max_features=MAX_NB_WORDS,tokenizer = tokenizer_)\n",
    "    return pipeline(train_docs, test_docs, vectorizer, lda_model)\n",
    "\n",
    "#to run\n",
    "basic_pp_exp1, basic_tw_exp1 = experiment_1(docs, SimpleTokenizer)\n",
    "stem_pp_exp1, stem_tw_exp1 = experiment_1(docs, StemTokenizer())\n",
    "lemma_pp_exp1, lemma_tw_exp1 = experiment_1(docs, LemmaTokenizer())\n",
    "```\n",
    "   \n",
    "In this notebook I will describe in detail experiment_1 and I will go faster through experiment_2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: CountVectorizer with different tokenizers\n",
    "\n",
    "Here I will simply use 3 tokenizers: a simple tokenizer (lowercases, tokenizes and removes stopwords), a \"StemTokenizer\" which applies stemization to the documents, and a \"LemmaTokenizer\" which applies lemmatization. Normally, one would use Lemmatization because the results are easier to interpret. This is the code for the tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "def SimpleTokenizer(doc):\n",
    "    \"\"\"Basic tokenizer using gensim's simple_preprocess\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    docs (list): list of documents\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    tokenized documents\n",
    "    \"\"\"\n",
    "    return [t for t in simple_preprocess(doc, min_len=3) if t not in STOPWORDS]\n",
    "\n",
    "\n",
    "class StemTokenizer(object):\n",
    "    \"\"\"Stem tokens in a document\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    docs (list): list of documents\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of stemmed tokens\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in SimpleTokenizer(doc)]\n",
    "\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    \"\"\"Lemmatize tokens in a document\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    docs (list): list of documents\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of lemmatized tokens\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.lemmatizer.lemmatize(t, pos=\"v\") for t in SimpleTokenizer(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the repo these classes are in the `nlp_utils.py` scrip and are imported from there\n",
    "\n",
    "Now let's define our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Lda model that will be used through all the experiments\n",
    "NB_TOPICS = 10\n",
    "lda_model = LatentDirichletAllocation(\n",
    "    n_components=NB_TOPICS,\n",
    "    learning_method='online',\n",
    "    max_iter=10,\n",
    "    batch_size=2000,\n",
    "    verbose=1,\n",
    "    max_doc_update_iter=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some might be saying: *\"wait, sklearn LDA? why not the implemenation at the well known and wonderfull gensim package\".* Fair question, you can go and have a look to the scrip `LDA_gensim_vs_sklearn.ipynb` or for more information. \n",
    "\n",
    "And now, this is how the experiment goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:812: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if doc_topic_distr != 'deprecated':\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0-train/test split\n",
    "MAX_NB_WORDS = 20000\n",
    "train_docs, test_docs = train_test_split(docs, test_size=0.25, random_state=0)\n",
    "vectorizer = CountVectorizer(min_df=10, max_df=0.5, max_features=MAX_NB_WORDS, tokenizer = LemmaTokenizer())\n",
    "\n",
    "# 1-vectorize\n",
    "tr_corpus = vectorizer.fit_transform(train_docs)\n",
    "te_corpus = vectorizer.transform(test_docs)\n",
    "n_words = len(vectorizer.vocabulary_)\n",
    "\n",
    "# 2-train model\n",
    "model = lda_model.fit(tr_corpus)\n",
    "\n",
    "# 3-compute perplexity\n",
    "gamma = model.transform(te_corpus)\n",
    "perplexity = model.perplexity(te_corpus, gamma)/n_words\n",
    "\n",
    "# 4-get vocabulary and return top N words. Let's 1st define a little helper\n",
    "def get_topic_words(topic_model, feature_names, n_top_words):\n",
    "    \"\"\"Helper to get n_top_words per topic\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    topic_model: LDA model\n",
    "    feature_names: vocabulary\n",
    "    n_top_words: number of top words to retrieve\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    topics: list of tuples with topic index, the most probable words and the scores/probs\n",
    "    \"\"\"\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(topic_model.components_):\n",
    "        topic_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        tot_score = np.sum(topic)\n",
    "        scores = [topic[i]/tot_score for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append([topic_idx, zip(topic_words, scores)])\n",
    "    return topics\n",
    "\n",
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "top_words = get_topic_words(model,features,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look to the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the LDA model with Lemmatization-preprocessing: 0.266592870347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  [(u'edu', 0.0067918785580433324),\n",
       "   (u'article', 0.0067569641886425745),\n",
       "   (u'think', 0.0065112375141850448),\n",
       "   (u'people', 0.0062470350411229641),\n",
       "   (u'know', 0.0052563821487041345),\n",
       "   (u'time', 0.0050749510534372905),\n",
       "   (u'like', 0.0048143805076149727),\n",
       "   (u'com', 0.0045819950601443246),\n",
       "   (u'post', 0.0038861505854044542),\n",
       "   (u'use', 0.0038454649715609765)]],\n",
       " [1,\n",
       "  [(u'edu', 0.010447325839301027),\n",
       "   (u'article', 0.0079042426298446666),\n",
       "   (u'like', 0.0074978798166720423),\n",
       "   (u'com', 0.0074467939943020254),\n",
       "   (u'think', 0.0067673233063638314),\n",
       "   (u'car', 0.0064237856975015864),\n",
       "   (u'go', 0.0058587958527275708),\n",
       "   (u'know', 0.0054723931238338875),\n",
       "   (u'get', 0.0047341178454663214),\n",
       "   (u'time', 0.004604247531864154)]],\n",
       " [2,\n",
       "  [(u'space', 0.010835665044824305),\n",
       "   (u'israel', 0.0088573078069718418),\n",
       "   (u'nasa', 0.006539412054020137),\n",
       "   (u'israeli', 0.0053793226519720716),\n",
       "   (u'jews', 0.0053435897725169776),\n",
       "   (u'launch', 0.0052689609705854658),\n",
       "   (u'new', 0.0050298852192734739),\n",
       "   (u'state', 0.0048659202772854196),\n",
       "   (u'edu', 0.0047185614090003818),\n",
       "   (u'center', 0.0038936904188903482)]],\n",
       " [3,\n",
       "  [(u'max', 0.025709629319151408),\n",
       "   (u'people', 0.011633278190246777),\n",
       "   (u'edu', 0.009579745776790851),\n",
       "   (u'article', 0.0093173356180936955),\n",
       "   (u'right', 0.0088558891881316115),\n",
       "   (u'gun', 0.0080962134416648745),\n",
       "   (u'think', 0.0069371469079681182),\n",
       "   (u'say', 0.0068669735522816737),\n",
       "   (u'com', 0.0059237497296155111),\n",
       "   (u'know', 0.0054840192686712768)]],\n",
       " [4,\n",
       "  [(u'key', 0.021863057143450534),\n",
       "   (u'government', 0.0095945553289037196),\n",
       "   (u'chip', 0.0092950916165653024),\n",
       "   (u'encryption', 0.0090235539971814729),\n",
       "   (u'use', 0.0078672513646673835),\n",
       "   (u'com', 0.0073409214876405072),\n",
       "   (u'clipper', 0.007190822282140687),\n",
       "   (u'public', 0.006515263983003749),\n",
       "   (u'security', 0.0064639371057630384),\n",
       "   (u'information', 0.0058823210511861528)]],\n",
       " [5,\n",
       "  [(u'god', 0.019024127365586264),\n",
       "   (u'people', 0.0085842138869310965),\n",
       "   (u'know', 0.0083044750404396971),\n",
       "   (u'say', 0.0082396797755770156),\n",
       "   (u'jesus', 0.0079707413369224014),\n",
       "   (u'think', 0.0072808433855805505),\n",
       "   (u'believe', 0.007141232017419381),\n",
       "   (u'come', 0.0055837823369723942),\n",
       "   (u'edu', 0.0053752422726811759),\n",
       "   (u'time', 0.005131434128369282)]],\n",
       " [6,\n",
       "  [(u'com', 0.0268123626297424),\n",
       "   (u'edu', 0.023892969718090597),\n",
       "   (u'article', 0.019833496230627093),\n",
       "   (u'sex', 0.0091073435837042156),\n",
       "   (u'homosexual', 0.0081705944683464014),\n",
       "   (u'men', 0.0081070479864166628),\n",
       "   (u'party', 0.0080325853052579319),\n",
       "   (u'gay', 0.0077215503448685293),\n",
       "   (u'state', 0.0074057997604392405),\n",
       "   (u'cramer', 0.007329431793848506)]],\n",
       " [7,\n",
       "  [(u'game', 0.015136288201599693),\n",
       "   (u'team', 0.010619975215757291),\n",
       "   (u'play', 0.0082365229364732822),\n",
       "   (u'edu', 0.0078657653894732456),\n",
       "   (u'win', 0.0073844964833550183),\n",
       "   (u'year', 0.0061056872795730386),\n",
       "   (u'turkish', 0.0059109159992362026),\n",
       "   (u'armenian', 0.0053354122208386636),\n",
       "   (u'hockey', 0.0049298358111017557),\n",
       "   (u'article', 0.0046673305179769026)]],\n",
       " [8,\n",
       "  [(u'edu', 0.014309173099561355),\n",
       "   (u'com', 0.0087318157203985104),\n",
       "   (u'drive', 0.0087232395123349703),\n",
       "   (u'know', 0.0074813530805775807),\n",
       "   (u'like', 0.0070659189759690174),\n",
       "   (u'work', 0.0069718743606462493),\n",
       "   (u'article', 0.0069175501344444562),\n",
       "   (u'card', 0.0066272969335348219),\n",
       "   (u'use', 0.0065961899337539038),\n",
       "   (u'run', 0.0060655304352365816)]],\n",
       " [9,\n",
       "  [(u'file', 0.014737103904477246),\n",
       "   (u'program', 0.0096556778032203666),\n",
       "   (u'edu', 0.0096368914606007796),\n",
       "   (u'image', 0.0086036449461412388),\n",
       "   (u'use', 0.0076119929481044124),\n",
       "   (u'mail', 0.0062524614364779406),\n",
       "   (u'windows', 0.0053125868145007518),\n",
       "   (u'include', 0.0052800042957376371),\n",
       "   (u'available', 0.0052420435618148499),\n",
       "   (u'ftp', 0.005115159040600374)]]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Perplexity of the LDA model with Lemmatization-preprocessing: {}\".format(perplexity))\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding to the `perplexity`, if all the words in our vocabulary would occur with equal probability, then the perplexity would be 1.0. It would be really hard to, given some words, associate them with a topic. In other words, in terms of finding topics, we would be really confused, perplexed! Therefore, the lower the perplexity, the better the model. \n",
    "\n",
    "However, [Chang et al., 2009](https://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf) showed that perplexity is not always correlated with topic intrepretability (and when you are doing topic modelling, you might want your topics to be as interpretable as possible). In this context [Röder et al, 2015](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) proposed a pipeline to estimate the so called `topic Coherence` that proved to be better correlated with human generated topic rankings. The `Coherence` metric is implemented in gensim, and I found another \"didactic\" implementation [here](https://github.com/jhlau/topic_interpretability). \n",
    "\n",
    "In a broad brush sense, there are different ways of estimating the `Coherence`, but are all based on probabilities of word co-occurrence. The way I understand the process described in their paper is as follows: \n",
    "\n",
    "1. Segmentation of a set into a smaller sets (e.g word pairs) \n",
    "2. Calculate the so-called Confirmation Measures that score the agreement of a given pair (e.g. the pointwise mutual information (PMI) or the normalized PMI (NPMI)). Confirmation Measures use word and word co-occurrence probailities (see [Röder et al, 2015](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) their expression (1) and (2) for example)\n",
    "3. Aggregation of the Confirmation Measures\n",
    "\n",
    "If we refer to the set of segmentations as $S$, the set of confirmation meassures as $M$, the set of word probabilities as $P$ and the aggregations functions as $\\Sigma$, Röder and collaborators define their coherence pipeline as $C = S \\times M \\times P \\times \\Sigma$\n",
    "\n",
    "I experimented a bit with Coherence and that can be found in `LDA_coherence_demo.ipynb`.\n",
    "\n",
    "Now let's go back to our code/experiments. I have run experiment_1 with a `LemmaTokenizer`. The same can be done with the `SimpleTokenizer` and `StemTokenizer` by simply changing the `tokenizer` parameter in `CountVectorizer`. For now, let's move to the 2nd experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: CountVectorizer with different tokenizers and bigrams\n",
    "\n",
    "The code here is mostly identical, we just need to introduce a new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "class Bigram(object):\n",
    "    \"\"\"Bigrams to get phrases like artificial_intelligence\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    docs (list): list of documents\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    the document with bigrams appended at the end\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.phraser = Phraser\n",
    "    def __call__(self, docs):\n",
    "        phrases = Phrases(docs,min_count=20)\n",
    "        bigram = self.phraser(phrases)\n",
    "        for idx in range(len(docs)):\n",
    "            for token in bigram[docs[idx]]:\n",
    "                if '_' in token:\n",
    "                    docs[idx].append(token)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, using again `LemmaTokenizer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    min_df=10, max_df=0.5,\n",
    "    max_features=MAX_NB_WORDS,\n",
    "    preprocessor = lambda x: x,\n",
    "    tokenizer = lambda x: x)\n",
    "tokenizer_ = LemmaTokenizer()\n",
    "tokens = [tokenizer_(doc) for doc in docs]\n",
    "phraser_ = Bigram()\n",
    "ptokens = phraser_(tokens)\n",
    "\n",
    "# 0-train/test split\n",
    "train_docs, test_docs = train_test_split(ptokens, test_size=0.25, random_state=0)\n",
    "\n",
    "# 1-vectorize\n",
    "tr_corpus = vectorizer.fit_transform(train_docs)\n",
    "te_corpus = vectorizer.transform(test_docs)\n",
    "n_words = len(vectorizer.vocabulary_)\n",
    "\n",
    "# 2-train model\n",
    "model = lda_model.fit(tr_corpus)\n",
    "\n",
    "# 3-compute perplexity\n",
    "gamma = model.transform(te_corpus)\n",
    "perplexity = model.perplexity(te_corpus, gamma)/n_words\n",
    "\n",
    "# 4-get vocabulary and return top N words.\n",
    "features = vectorizer.get_feature_names()\n",
    "top_words = get_topic_words(model,features,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.258999126731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  [(u'file', 0.014911280157244338),\n",
       "   (u'image', 0.010087368004218188),\n",
       "   (u'edu', 0.009198718391388392),\n",
       "   (u'program', 0.0086550028771869639),\n",
       "   (u'use', 0.0075488499161402078),\n",
       "   (u'window', 0.0058757980358187844),\n",
       "   (u'graphics', 0.0054010947879012941),\n",
       "   (u'windows', 0.0050675226372621073),\n",
       "   (u'include', 0.0050662931002265869),\n",
       "   (u'display', 0.0047945041399675989)]],\n",
       " [1,\n",
       "  [(u'people', 0.007344598485807553),\n",
       "   (u'state', 0.0059917967001673874),\n",
       "   (u'right', 0.0056321028217693097),\n",
       "   (u'say', 0.0044455979349985386),\n",
       "   (u'government', 0.0044290789455164407),\n",
       "   (u'think', 0.0042957762398633916),\n",
       "   (u'israel', 0.0041167481937032209),\n",
       "   (u'president', 0.0038552756959020014),\n",
       "   (u'time', 0.003481948886547852),\n",
       "   (u'work', 0.0033615996187893503)]],\n",
       " [2,\n",
       "  [(u'use', 0.0068769583314697205),\n",
       "   (u'space', 0.0067398018754097223),\n",
       "   (u'chip', 0.0059586126831805168),\n",
       "   (u'key', 0.005910233879818063),\n",
       "   (u'scsi', 0.0047015105484066629),\n",
       "   (u'article', 0.004611000835516208),\n",
       "   (u'drive', 0.0044445219430845088),\n",
       "   (u'com', 0.0044018669944790251),\n",
       "   (u'power', 0.0043997923515706689),\n",
       "   (u'time', 0.004174449175112763)]],\n",
       " [3,\n",
       "  [(u'article', 0.0095376084349709043),\n",
       "   (u'com', 0.0079617616160023285),\n",
       "   (u'edu', 0.0071311353546955232),\n",
       "   (u'value', 0.0070080118216074979),\n",
       "   (u'people', 0.0059828688453655351),\n",
       "   (u'objective', 0.0058980105587414147),\n",
       "   (u'science', 0.0053187850949120681),\n",
       "   (u'know', 0.0052624377385609008),\n",
       "   (u'health', 0.0051018891643295526),\n",
       "   (u'think', 0.0049745861212630209)]],\n",
       " [4,\n",
       "  [(u'edu', 0.014829603042289322),\n",
       "   (u'card', 0.0096802959648589742),\n",
       "   (u'work', 0.0075482991971951739),\n",
       "   (u'know', 0.0075037634961681926),\n",
       "   (u'drive', 0.0072862270371747886),\n",
       "   (u'like', 0.006491795416164757),\n",
       "   (u'com', 0.0063389472667433446),\n",
       "   (u'thank', 0.0061036318302007085),\n",
       "   (u'price', 0.0056718963718110967),\n",
       "   (u'use', 0.0056706562751180131)]],\n",
       " [5,\n",
       "  [(u'mail', 0.011686562374234738),\n",
       "   (u'com', 0.010174185973320192),\n",
       "   (u'edu', 0.0089762639654460824),\n",
       "   (u'know', 0.0071223066705190874),\n",
       "   (u'key', 0.0067334979322390661),\n",
       "   (u'post', 0.0064795631237795793),\n",
       "   (u'list', 0.0064434526700839862),\n",
       "   (u'thank', 0.0063985686581667601),\n",
       "   (u'do', 0.0058229239304426559),\n",
       "   (u'software', 0.005711480687587497)]],\n",
       " [6,\n",
       "  [(u'edu', 0.014704490412494114),\n",
       "   (u'article', 0.009342145150198931),\n",
       "   (u'people', 0.0082788560903488111),\n",
       "   (u'gun', 0.0081194832046558568),\n",
       "   (u'com', 0.0072576992280261121),\n",
       "   (u'think', 0.0058708418276976161),\n",
       "   (u'right', 0.0055646788627262116),\n",
       "   (u'know', 0.0052171541910384731),\n",
       "   (u'say', 0.0051266676178687342),\n",
       "   (u'like', 0.004453996349151149)]],\n",
       " [7,\n",
       "  [(u'team', 0.013960331260875116),\n",
       "   (u'play', 0.011421479388732508),\n",
       "   (u'hockey', 0.0087135376908440594),\n",
       "   (u'win', 0.0085512907639745048),\n",
       "   (u'pit', 0.0080418919609222973),\n",
       "   (u'game', 0.0071268077746253688),\n",
       "   (u'period', 0.0061963893778835804),\n",
       "   (u'det', 0.0059968373922422756),\n",
       "   (u'pts', 0.0058624323412704632),\n",
       "   (u'score', 0.0056218186822330788)]],\n",
       " [8,\n",
       "  [(u'god', 0.016868821273284998),\n",
       "   (u'people', 0.0086785094371431126),\n",
       "   (u'think', 0.0077748330955455325),\n",
       "   (u'know', 0.007660013446555358),\n",
       "   (u'jesus', 0.0070733963808783133),\n",
       "   (u'say', 0.0068377298615630699),\n",
       "   (u'believe', 0.0067533569618950301),\n",
       "   (u'mean', 0.0050720001868191505),\n",
       "   (u'edu', 0.005016345390153591),\n",
       "   (u'time', 0.0047586593483477787)]],\n",
       " [9,\n",
       "  [(u'max', 0.020981390003361983),\n",
       "   (u'edu', 0.011989831287644888),\n",
       "   (u'article', 0.0090618271406779551),\n",
       "   (u'game', 0.0081689524303117135),\n",
       "   (u'max_max', 0.008134620511461018),\n",
       "   (u'com', 0.0077829595033795142),\n",
       "   (u'like', 0.0074130394964551163),\n",
       "   (u'think', 0.0072715962507460506),\n",
       "   (u'time', 0.0056605307291918375),\n",
       "   (u'car', 0.0054532513834692417)]]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(perplexity)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results but slightly lower perplexity. And finally\n",
    "\n",
    "## Experiment 3: Filtering words\n",
    "\n",
    "Here we will use a vocabulary to filter some words. To my experience, these approaches do not work well (lead to higher perplexities) but...worth trying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:816: DeprecationWarning: Argument 'doc_topic_distr' is deprecated and is being ignored as of 0.19. Support for this argument will be removed in 0.21.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "class WordFilter(object):\n",
    "    \"\"\"Filter words based on a vocabulary\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    vocab: the vocabulary used for filtering\n",
    "    doc  : the document containing the tokens to be filtered\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    filetered document\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.filter = vocab\n",
    "    def __call__(self, doc):\n",
    "        return [t for t in doc if t in self.filter]\n",
    "\n",
    "\n",
    "wordfilter = WordFilter(vocab=set(words.words()))\n",
    "tokens = [SimpleTokenizer(doc) for doc in docs]\n",
    "ftokens = [wordfilter(d) for d in tokens]\n",
    "\n",
    "# 0-train/test split\n",
    "train_docs, test_docs = train_test_split(ftokens, test_size=0.25, random_state=0)\n",
    "\n",
    "# 1-vectorize\n",
    "tr_corpus = vectorizer.fit_transform(train_docs)\n",
    "te_corpus = vectorizer.transform(test_docs)\n",
    "n_words = len(vectorizer.vocabulary_)\n",
    "\n",
    "# 2-train model\n",
    "model = lda_model.fit(tr_corpus)\n",
    "\n",
    "# 3-compute perplexity\n",
    "gamma = model.transform(te_corpus)\n",
    "perplexity = model.perplexity(te_corpus, gamma)/n_words\n",
    "\n",
    "# 4-get vocabulary and return top N words.\n",
    "features = vectorizer.get_feature_names()\n",
    "top_words = get_topic_words(model,features,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342266878772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  [(u'article', 0.017580034444793155),\n",
       "   (u'objective', 0.013131849459992533),\n",
       "   (u'science', 0.01221008169911989),\n",
       "   (u'frank', 0.01086650145166858),\n",
       "   (u'moral', 0.010355460503028671),\n",
       "   (u'theory', 0.010050946200559904),\n",
       "   (u'think', 0.009480288761605591),\n",
       "   (u'people', 0.0091244576103224184),\n",
       "   (u'morality', 0.0083461139872245838),\n",
       "   (u'value', 0.0082145670489376515)]],\n",
       " [1,\n",
       "  [(u'game', 0.022896501711834081),\n",
       "   (u'team', 0.022492179496859718),\n",
       "   (u'year', 0.013789294025856455),\n",
       "   (u'hockey', 0.012704702048298417),\n",
       "   (u'play', 0.011010993838714867),\n",
       "   (u'win', 0.011007070437857393),\n",
       "   (u'new', 0.010558831078797752),\n",
       "   (u'season', 0.0098853327743381892),\n",
       "   (u'league', 0.0083729823937593265),\n",
       "   (u'pit', 0.0080520131850816464)]],\n",
       " [2,\n",
       "  [(u'turkey', 0.0314575726942741),\n",
       "   (u'day', 0.022628921534513388),\n",
       "   (u'article', 0.014152941958146139),\n",
       "   (u'sun', 0.012359529167810064),\n",
       "   (u'send', 0.0088843568817670894),\n",
       "   (u'law', 0.0077262802105048422),\n",
       "   (u'days', 0.0073007341319162771),\n",
       "   (u'ken', 0.0060492941111271355),\n",
       "   (u'sabbath', 0.0060459110572685599),\n",
       "   (u'patent', 0.0059561837609799946)]],\n",
       " [3,\n",
       "  [(u'use', 0.011379072607292958),\n",
       "   (u'file', 0.0099968093065111105),\n",
       "   (u'mail', 0.0079346542625576139),\n",
       "   (u'thanks', 0.0070593793695560613),\n",
       "   (u'know', 0.0068720921413181965),\n",
       "   (u'data', 0.0066277637539788447),\n",
       "   (u'program', 0.0066066207474252611),\n",
       "   (u'like', 0.0064432495279313122),\n",
       "   (u'available', 0.0061766442225587885),\n",
       "   (u'bit', 0.0056616584753016357)]],\n",
       " [4,\n",
       "  [(u'god', 0.024862756639366036),\n",
       "   (u'people', 0.012185863802130634),\n",
       "   (u'believe', 0.0082021967390423448),\n",
       "   (u'article', 0.0076835733533890272),\n",
       "   (u'think', 0.0074553247910282075),\n",
       "   (u'know', 0.0071740713205933919),\n",
       "   (u'like', 0.0060583009333443488),\n",
       "   (u'religion', 0.0059237964316780113),\n",
       "   (u'church', 0.0055549593665275227),\n",
       "   (u'time', 0.005203103337683119)]],\n",
       " [5,\n",
       "  [(u'know', 0.014852251292170516),\n",
       "   (u'like', 0.014519574816918925),\n",
       "   (u'article', 0.014282284801916529),\n",
       "   (u'people', 0.0127954095044548),\n",
       "   (u'think', 0.011767605225352374),\n",
       "   (u'time', 0.0090569576046052053),\n",
       "   (u'said', 0.0086734738405605163),\n",
       "   (u'going', 0.0077541332342834363),\n",
       "   (u'way', 0.006612280042201016),\n",
       "   (u'want', 0.0057912420886101698)]],\n",
       " [6,\n",
       "  [(u'car', 0.0099441246882610405),\n",
       "   (u'like', 0.0099259465539428913),\n",
       "   (u'article', 0.0090002956396173107),\n",
       "   (u'good', 0.0074506006398947109),\n",
       "   (u'new', 0.0073254106235908483),\n",
       "   (u'power', 0.0058053206141985619),\n",
       "   (u'know', 0.0053797585994644192),\n",
       "   (u'space', 0.0052948419859413154),\n",
       "   (u'price', 0.0052841336639920687),\n",
       "   (u'use', 0.0051101507089231423)]],\n",
       " [7,\n",
       "  [(u'new', 0.0075737712318973555),\n",
       "   (u'research', 0.0071661692502135617),\n",
       "   (u'space', 0.0062520480968323404),\n",
       "   (u'university', 0.0062120085370534397),\n",
       "   (u'health', 0.0055372393185892741),\n",
       "   (u'people', 0.0051497953261631169),\n",
       "   (u'information', 0.0051049017650127773),\n",
       "   (u'world', 0.0047789415386120908),\n",
       "   (u'medical', 0.0047685620703892782),\n",
       "   (u'center', 0.0046953173084207239)]],\n",
       " [8,\n",
       "  [(u'article', 0.019114188675560102),\n",
       "   (u'like', 0.010051611035440075),\n",
       "   (u'bike', 0.0097829414041843629),\n",
       "   (u'think', 0.0093443255332753949),\n",
       "   (u'dod', 0.0081103408625671659),\n",
       "   (u'good', 0.0078087643905318336),\n",
       "   (u'hit', 0.0076500196311774047),\n",
       "   (u'time', 0.007346120028319145),\n",
       "   (u'right', 0.0065574780780695641),\n",
       "   (u'year', 0.0056309892124635991)]],\n",
       " [9,\n",
       "  [(u'people', 0.013418052312996037),\n",
       "   (u'government', 0.011697466528551782),\n",
       "   (u'article', 0.0085876752953535328),\n",
       "   (u'right', 0.0070015019316086551),\n",
       "   (u'law', 0.0065617028611255858),\n",
       "   (u'think', 0.0064393431166479284),\n",
       "   (u'gun', 0.0055903156727453805),\n",
       "   (u'key', 0.0050403817692795316),\n",
       "   (u'like', 0.0050057580597930385),\n",
       "   (u'state', 0.0047134643066192071)]]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(perplexity)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is worse and the topic words do not look much better. Anyway...I have run the three experiments with the 3 tokenizers and 3 different number of topics values: 10, 20 and 50. For example:  \n",
    "\n",
    "```\n",
    "python lda_perplexity.py --n_topics 10\n",
    "```\n",
    "\n",
    "The results are stored in dictionaries (pickle files). In the `LDA_coherence_demo.ipynb` notebook I have used gensim to find the coherence of the topics for the top 3 models. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
