{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC COHERENCE\n",
    "\n",
    "As I mentioned in `LDA_perplexity_demo.ipynb` [Chang et al., 2009](https://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf) found that perplexity is not always correlated with topic intrepretability. In this context [Röder et al, 2015](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) proposed a pipeline to estimate the so called `topic Coherence` that proved to be better correlated with human generated topic rankings. The `Coherence` metric is implemented in gensim through the `CoherenceModel` method.\n",
    "\n",
    "Just as a reminder, the way I understand the process described in their paper is as follows: \n",
    "\n",
    "1. Segmentation of a set into a smaller sets (e.g word pairs) \n",
    "2. Calculate the Confirmation Measures that scores the agreement of a given pair (e.g. the pointwise mutual information (PMI) or the normalized PMI (NPMI)). Confirmation Measures use word and word co-occurrence probailities (see [Röder et al, 2015](http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf) their expression (1) and (2) for example)\n",
    "3. Aggregation of the Confirmation Measures\n",
    "\n",
    "Remember that I run `lda_perplexity.py` with 10, 20 and 50 topics for the 3 experiments that I described in `LDA_perplexity_demo.ipynb`. I saved the results in picke files in a directory called `data_processed`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'basic_exp1': 0.31471618171353233,\n",
       " 'basic_exp2': 0.31429573959863427,\n",
       " 'glove': 0.32085378021600947,\n",
       " 'lemma_exp1': 0.26659287034652801,\n",
       " 'lemma_exp2': 0.25899912673071496,\n",
       " 'stem_exp1': 0.26658473872097244,\n",
       " 'stem_exp2': 0.25414921201102364,\n",
       " 'words': 0.34226687877222595}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "\n",
    "# dictionaries with the perplexity values per set-up\n",
    "pp_10 = pickle.load(open(\"data_processed/perplexity_10.p\", \"rb\"))\n",
    "pp_20 = pickle.load(open(\"data_processed/perplexity_20.p\", \"rb\"))\n",
    "pp_50 = pickle.load(open(\"data_processed/perplexity_50.p\", \"rb\"))\n",
    "\n",
    "# dictionaries with dataframes with words per topic per set-up\n",
    "tw_10 = pickle.load(open(\"data_processed/topic_words_df_10.p\", \"rb\"))\n",
    "tw_20 = pickle.load(open(\"data_processed/topic_words_df_20.p\", \"rb\"))\n",
    "tw_50 = pickle.load(open(\"data_processed/topic_words_df_50.p\", \"rb\"))\n",
    "\n",
    "# Let's have a look\n",
    "pp_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the keys are the different set-ups we used. These are\n",
    "\n",
    "`basic_exp1`: `SimpleTokenizer` without bigrams\n",
    "\n",
    "`basic_exp2`: `SimpleTokenizer` with bigrams\n",
    "\n",
    "`stem_exp1`: `StemTokenizer` without bigrams\n",
    "\n",
    "`stem_exp2`: `StemTokenizer` with bigrams\n",
    "\n",
    "`lemma_exp1`: `LemmaTokenizer` without bigrams\n",
    "\n",
    "`lemma_exp2`: `LemmaTokenizer` with bigrams\n",
    "\n",
    "`words`: filtering words using nltk.words vocabulary\n",
    "\n",
    "`glove`: filering words using the 400k tokens corresponding to the well known glove vectors\n",
    "\n",
    "Let's have a look to some of the dataframes with words per topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>edu</td>\n",
       "      <td>state</td>\n",
       "      <td>power</td>\n",
       "      <td>pay</td>\n",
       "      <td>team</td>\n",
       "      <td>edu</td>\n",
       "      <td>people</td>\n",
       "      <td>pit</td>\n",
       "      <td>god</td>\n",
       "      <td>max</td>\n",
       "      <td>drug</td>\n",
       "      <td>edu</td>\n",
       "      <td>drive</td>\n",
       "      <td>appear</td>\n",
       "      <td>think</td>\n",
       "      <td>edu</td>\n",
       "      <td>israel</td>\n",
       "      <td>edu</td>\n",
       "      <td>key</td>\n",
       "      <td>file</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article</td>\n",
       "      <td>government</td>\n",
       "      <td>water</td>\n",
       "      <td>insurance</td>\n",
       "      <td>san</td>\n",
       "      <td>article</td>\n",
       "      <td>gun</td>\n",
       "      <td>det</td>\n",
       "      <td>jesus</td>\n",
       "      <td>game</td>\n",
       "      <td>medical</td>\n",
       "      <td>com</td>\n",
       "      <td>edu</td>\n",
       "      <td>party</td>\n",
       "      <td>people</td>\n",
       "      <td>article</td>\n",
       "      <td>jews</td>\n",
       "      <td>article</td>\n",
       "      <td>value</td>\n",
       "      <td>program</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apr</td>\n",
       "      <td>people</td>\n",
       "      <td>light</td>\n",
       "      <td>edu</td>\n",
       "      <td>hockey</td>\n",
       "      <td>blue</td>\n",
       "      <td>edu</td>\n",
       "      <td>pts</td>\n",
       "      <td>christ</td>\n",
       "      <td>max_max</td>\n",
       "      <td>cause</td>\n",
       "      <td>article</td>\n",
       "      <td>windows</td>\n",
       "      <td>art</td>\n",
       "      <td>god</td>\n",
       "      <td>uiuc</td>\n",
       "      <td>israeli</td>\n",
       "      <td>black</td>\n",
       "      <td>chip</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harvard</td>\n",
       "      <td>right</td>\n",
       "      <td>grind</td>\n",
       "      <td>abortion</td>\n",
       "      <td>game</td>\n",
       "      <td>org</td>\n",
       "      <td>say</td>\n",
       "      <td>chi</td>\n",
       "      <td>say</td>\n",
       "      <td>team</td>\n",
       "      <td>know</td>\n",
       "      <td>car</td>\n",
       "      <td>work</td>\n",
       "      <td>men</td>\n",
       "      <td>know</td>\n",
       "      <td>henry</td>\n",
       "      <td>article</td>\n",
       "      <td>com</td>\n",
       "      <td>encryption</td>\n",
       "      <td>edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>turkey</td>\n",
       "      <td>president</td>\n",
       "      <td>like</td>\n",
       "      <td>health</td>\n",
       "      <td>edu</td>\n",
       "      <td>com</td>\n",
       "      <td>article</td>\n",
       "      <td>period</td>\n",
       "      <td>lord</td>\n",
       "      <td>play</td>\n",
       "      <td>article</td>\n",
       "      <td>like</td>\n",
       "      <td>know</td>\n",
       "      <td>new</td>\n",
       "      <td>believe</td>\n",
       "      <td>umd</td>\n",
       "      <td>jewish</td>\n",
       "      <td>apr</td>\n",
       "      <td>objective</td>\n",
       "      <td>image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adam</td>\n",
       "      <td>think</td>\n",
       "      <td>heat</td>\n",
       "      <td>tax</td>\n",
       "      <td>new</td>\n",
       "      <td>pitt</td>\n",
       "      <td>kill</td>\n",
       "      <td>bos</td>\n",
       "      <td>sin</td>\n",
       "      <td>year</td>\n",
       "      <td>doctor</td>\n",
       "      <td>apr</td>\n",
       "      <td>use</td>\n",
       "      <td>wolverine</td>\n",
       "      <td>mean</td>\n",
       "      <td>uiuc_edu</td>\n",
       "      <td>com</td>\n",
       "      <td>hole</td>\n",
       "      <td>com</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>com</td>\n",
       "      <td>work</td>\n",
       "      <td>earth</td>\n",
       "      <td>berkeley</td>\n",
       "      <td>league</td>\n",
       "      <td>pitt_edu</td>\n",
       "      <td>think</td>\n",
       "      <td>van</td>\n",
       "      <td>come</td>\n",
       "      <td>think</td>\n",
       "      <td>use</td>\n",
       "      <td>new</td>\n",
       "      <td>com</td>\n",
       "      <td>university</td>\n",
       "      <td>say</td>\n",
       "      <td>toronto</td>\n",
       "      <td>arab</td>\n",
       "      <td>muslims</td>\n",
       "      <td>article</td>\n",
       "      <td>mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bnr</td>\n",
       "      <td>new</td>\n",
       "      <td>energy</td>\n",
       "      <td>coverage</td>\n",
       "      <td>nhl</td>\n",
       "      <td>duke</td>\n",
       "      <td>right</td>\n",
       "      <td>tor</td>\n",
       "      <td>life</td>\n",
       "      <td>edu</td>\n",
       "      <td>disease</td>\n",
       "      <td>think</td>\n",
       "      <td>card</td>\n",
       "      <td>political</td>\n",
       "      <td>edu</td>\n",
       "      <td>cso</td>\n",
       "      <td>edu</td>\n",
       "      <td>stanford</td>\n",
       "      <td>clipper</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>arromdee</td>\n",
       "      <td>law</td>\n",
       "      <td>radar</td>\n",
       "      <td>cost</td>\n",
       "      <td>season</td>\n",
       "      <td>like</td>\n",
       "      <td>know</td>\n",
       "      <td>buf</td>\n",
       "      <td>know</td>\n",
       "      <td>win</td>\n",
       "      <td>time</td>\n",
       "      <td>bike</td>\n",
       "      <td>like</td>\n",
       "      <td>man</td>\n",
       "      <td>like</td>\n",
       "      <td>space</td>\n",
       "      <td>right</td>\n",
       "      <td>know</td>\n",
       "      <td>frank</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>article_apr</td>\n",
       "      <td>time</td>\n",
       "      <td>star</td>\n",
       "      <td>private</td>\n",
       "      <td>win</td>\n",
       "      <td>tek</td>\n",
       "      <td>fbi</td>\n",
       "      <td>min</td>\n",
       "      <td>love</td>\n",
       "      <td>time</td>\n",
       "      <td>effect</td>\n",
       "      <td>know</td>\n",
       "      <td>run</td>\n",
       "      <td>annual</td>\n",
       "      <td>article</td>\n",
       "      <td>cobb</td>\n",
       "      <td>people</td>\n",
       "      <td>time</td>\n",
       "      <td>science</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic_0     topic_1 topic_2    topic_3 topic_4   topic_5  topic_6  \\\n",
       "0          edu       state   power        pay    team       edu   people   \n",
       "1      article  government   water  insurance     san   article      gun   \n",
       "2          apr      people   light        edu  hockey      blue      edu   \n",
       "3      harvard       right   grind   abortion    game       org      say   \n",
       "4       turkey   president    like     health     edu       com  article   \n",
       "5         adam       think    heat        tax     new      pitt     kill   \n",
       "6          com        work   earth   berkeley  league  pitt_edu    think   \n",
       "7          bnr         new  energy   coverage     nhl      duke    right   \n",
       "8     arromdee         law   radar       cost  season      like     know   \n",
       "9  article_apr        time    star    private     win       tek      fbi   \n",
       "\n",
       "  topic_7 topic_8  topic_9 topic_10 topic_11 topic_12    topic_13 topic_14  \\\n",
       "0     pit     god      max     drug      edu    drive      appear    think   \n",
       "1     det   jesus     game  medical      com      edu       party   people   \n",
       "2     pts  christ  max_max    cause  article  windows         art      god   \n",
       "3     chi     say     team     know      car     work         men     know   \n",
       "4  period    lord     play  article     like     know         new  believe   \n",
       "5     bos     sin     year   doctor      apr      use   wolverine     mean   \n",
       "6     van    come    think      use      new      com  university      say   \n",
       "7     tor    life      edu  disease    think     card   political      edu   \n",
       "8     buf    know      win     time     bike     like         man     like   \n",
       "9     min    love     time   effect     know      run      annual  article   \n",
       "\n",
       "   topic_15 topic_16  topic_17    topic_18   topic_19  \n",
       "0       edu   israel       edu         key       file  \n",
       "1   article     jews   article       value    program  \n",
       "2      uiuc  israeli     black        chip        use  \n",
       "3     henry  article       com  encryption        edu  \n",
       "4       umd   jewish       apr   objective      image  \n",
       "5  uiuc_edu      com      hole         com      space  \n",
       "6   toronto     arab   muslims     article       mail  \n",
       "7       cso      edu  stanford     clipper  available  \n",
       "8     space    right      know       frank    include  \n",
       "9      cobb   people      time     science       data  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_20['lemma_exp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file</td>\n",
       "      <td>edu</td>\n",
       "      <td>edu</td>\n",
       "      <td>game</td>\n",
       "      <td>edu</td>\n",
       "      <td>govern</td>\n",
       "      <td>articl</td>\n",
       "      <td>max</td>\n",
       "      <td>space</td>\n",
       "      <td>god</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>window</td>\n",
       "      <td>articl</td>\n",
       "      <td>drive</td>\n",
       "      <td>team</td>\n",
       "      <td>com</td>\n",
       "      <td>peopl</td>\n",
       "      <td>com</td>\n",
       "      <td>max_max</td>\n",
       "      <td>nasa</td>\n",
       "      <td>christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>program</td>\n",
       "      <td>pay</td>\n",
       "      <td>work</td>\n",
       "      <td>play</td>\n",
       "      <td>articl</td>\n",
       "      <td>right</td>\n",
       "      <td>edu</td>\n",
       "      <td>edu</td>\n",
       "      <td>year</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>use</td>\n",
       "      <td>insur</td>\n",
       "      <td>know</td>\n",
       "      <td>edu</td>\n",
       "      <td>mail</td>\n",
       "      <td>state</td>\n",
       "      <td>like</td>\n",
       "      <td>year</td>\n",
       "      <td>launch</td>\n",
       "      <td>believ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>edu</td>\n",
       "      <td>com</td>\n",
       "      <td>like</td>\n",
       "      <td>win</td>\n",
       "      <td>post</td>\n",
       "      <td>law</td>\n",
       "      <td>car</td>\n",
       "      <td>game</td>\n",
       "      <td>orbit</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>imag</td>\n",
       "      <td>abort</td>\n",
       "      <td>card</td>\n",
       "      <td>hockey</td>\n",
       "      <td>univers</td>\n",
       "      <td>key</td>\n",
       "      <td>think</td>\n",
       "      <td>hit</td>\n",
       "      <td>medic</td>\n",
       "      <td>jesu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>com</td>\n",
       "      <td>uiuc</td>\n",
       "      <td>use</td>\n",
       "      <td>player</td>\n",
       "      <td>group</td>\n",
       "      <td>gun</td>\n",
       "      <td>peopl</td>\n",
       "      <td>articl</td>\n",
       "      <td>use</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mail</td>\n",
       "      <td>tax</td>\n",
       "      <td>com</td>\n",
       "      <td>year</td>\n",
       "      <td>inform</td>\n",
       "      <td>think</td>\n",
       "      <td>thing</td>\n",
       "      <td>bhj</td>\n",
       "      <td>research</td>\n",
       "      <td>edu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>run</td>\n",
       "      <td>health</td>\n",
       "      <td>problem</td>\n",
       "      <td>articl</td>\n",
       "      <td>new</td>\n",
       "      <td>armenian</td>\n",
       "      <td>know</td>\n",
       "      <td>basebal</td>\n",
       "      <td>time</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>avail</td>\n",
       "      <td>want</td>\n",
       "      <td>thank</td>\n",
       "      <td>season</td>\n",
       "      <td>address</td>\n",
       "      <td>articl</td>\n",
       "      <td>good</td>\n",
       "      <td>pitch</td>\n",
       "      <td>effect</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_0 topic_1  topic_2 topic_3  topic_4   topic_5 topic_6  topic_7  \\\n",
       "0     file     edu      edu    game      edu    govern  articl      max   \n",
       "1   window  articl    drive    team      com     peopl     com  max_max   \n",
       "2  program     pay     work    play   articl     right     edu      edu   \n",
       "3      use   insur     know     edu     mail     state    like     year   \n",
       "4      edu     com     like     win     post       law     car     game   \n",
       "5     imag   abort     card  hockey  univers       key   think      hit   \n",
       "6      com    uiuc      use  player    group       gun   peopl   articl   \n",
       "7     mail     tax      com    year   inform     think   thing      bhj   \n",
       "8      run  health  problem  articl      new  armenian    know  basebal   \n",
       "9    avail    want    thank  season  address    articl    good    pitch   \n",
       "\n",
       "    topic_8    topic_9  \n",
       "0     space        god  \n",
       "1      nasa  christian  \n",
       "2      year      peopl  \n",
       "3    launch     believ  \n",
       "4     orbit       know  \n",
       "5     medic       jesu  \n",
       "6       use      think  \n",
       "7  research        edu  \n",
       "8      time       like  \n",
       "9    effect       time  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tw_10['stem_exp2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select the top 10 set-ups based on their perplexity and see how coherent they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lemma_exp2_10': 0.25899912673071496, 'stem_exp2_10': 0.25414921201102364}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_perplexities = pp_10.values() + pp_20.values() + pp_50.values()\n",
    "top10_cut = round(np.percentile(all_perplexities, 10), 2)\n",
    "top10_models = {}\n",
    "for n,p in zip([10,20,50], [pp_10,pp_20,pp_50]):\n",
    "    for model, perplexity in p.iteritems():\n",
    "        if perplexity <= top10_cut:\n",
    "            top10_models[\"_\".join([model,str(n)])] = perplexity\n",
    "top10_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 set-ups lead to similar perplexity. I have to say that normally, my (limited) experience is that Lemmatization and bigrams always leads to the best models (here with 10 topics).  \n",
    "\n",
    "Let's use `gensim` to get the coherence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel, LdaMulticore\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.corpora import Dictionary\n",
    "from nlp_utils import SimpleTokenizer, StemTokenizer, LemmaTokenizer\n",
    "from nlp_utils import Bigram, read_docs\n",
    "\n",
    "MAX_NB_WORDS = 20000\n",
    "TEXT_DATA_DIR = '/home/ubuntu/working/text_classification/20_newsgroup/'\n",
    "docs, doc_classes = read_docs(TEXT_DATA_DIR)\n",
    "\n",
    "# If you run this as a script: python lda_coherence.py \n",
    "# you might want loggings on.\n",
    "\n",
    "# logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.DEBUG)\n",
    "# logging.root.level = logging.DEBUG\n",
    "def model_builder(docs, tokenizer_, phaser_, nb_topics):\n",
    "    \"\"\"Simple helper so I don't have to repeat code\n",
    "    \"\"\"\n",
    "    doc_tokens  = [tokenizer_(doc) for doc in docs]\n",
    "    doc_tokens  = phaser_(doc_tokens)\n",
    "\n",
    "    id2word = Dictionary(doc_tokens)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.5, keep_n=MAX_NB_WORDS)\n",
    "    corpus = [id2word.doc2bow(doc) for doc in doc_tokens]\n",
    "\n",
    "    model = LdaMulticore(\n",
    "       corpus=corpus,\n",
    "       id2word=id2word,\n",
    "       decay=0.7,\n",
    "       offset=10.0,\n",
    "       num_topics=nb_topics,\n",
    "       passes=5,\n",
    "       batch=False,\n",
    "       chunksize=2000,\n",
    "       iterations=50)\n",
    "\n",
    "    return doc_tokens, id2word, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning! this will take some time (even on an AWS p2 instance)\n",
    "stem_10_texts, stem_10_dict, stem_10_model = model_builder(docs, StemTokenizer(), Bigram(), 10)\n",
    "lemma_10_texts, lemma_10_dict, lemma_10_model = model_builder(docs, LemmaTokenizer(), Bigram(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coherence with stemmization and 10 topics: 0.496838746061\n",
      "coherence with lemmatization and 10 topics: 0.526520456048\n"
     ]
    }
   ],
   "source": [
    "stem_10_CM = CoherenceModel(model=stem_10_model, texts=stem_10_texts, dictionary=stem_10_dict, coherence='c_v')\n",
    "stem_10_coherence = stem_10_CM.get_coherence()\n",
    "\n",
    "lemma_10_CM = CoherenceModel(model=lemma_10_model, texts=lemma_10_texts, dictionary=lemma_10_dict, coherence='c_v')\n",
    "lemma_10_coherence = lemma_10_CM.get_coherence()\n",
    "\n",
    "print(\"coherence with stemmization and 10 topics: {}\".format(stem_10_coherence))\n",
    "print(\"coherence with lemmatization and 10 topics: {}\".format(lemma_10_coherence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the most coherent model is indeed that built after preprocessing the data with lemmatization and bigrams. \n",
    "\n",
    "The final LDA model was built running:\n",
    "\n",
    "```\n",
    "python lda_build_model.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
